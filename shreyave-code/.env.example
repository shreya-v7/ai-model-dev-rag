# Copy this file to .env and fill in your own keys
# DO NOT commit .env to version control

# LLM Provider (grok or azure)
LLM_PROVIDER=grok

# Grok-3 (CMU LLM API)
GROK_API_KEY=YOUR_GROK_API_KEY_HERE
GROK_ENDPOINT=https://your-endpoint.services.ai.azure.com/openai/v1/
GROK_MODEL=grok-3

# Azure OpenAI
AZURE_ENDPOINT=https://your-resource.cognitiveservices.azure.com/
AZURE_API_KEY=YOUR_AZURE_API_KEY_HERE
AZURE_API_VERSION=2024-12-01-preview
AZURE_MODEL=o4-mini

# Generation
GENERATION_TEMPERATURE=0.2
JUDGE_TEMPERATURE=0.0
JUDGE_MAX_TOKENS=2000
DECOMPOSE_MAX_TOKENS=1000
REWRITE_MAX_TOKENS=500
DECOMPOSE_TEMPERATURE=0.0
REWRITE_TEMPERATURE=0.0

# Embeddings / Chunking / Retrieval
EMBED_MODEL_NAME=all-MiniLM-L6-v2
EMBED_BATCH_SIZE=32
CHUNK_SIZE_TOKENS=500
CHUNK_OVERLAP_TOKENS=100
TOP_K=5
ENHANCED_TOP_N=8
MAX_SUB_QUERIES=4

# Download / Serving
REQUEST_TIMEOUT=120
REQUEST_DELAY_S=2
API_HOST=0.0.0.0
API_PORT=8000
STREAMLIT_PORT=8501

# Evaluation
CHUNK_PREVIEW_LEN=200
SCORE_PASS_THRESHOLD=3.5
SCORE_WARN_THRESHOLD=2.5
